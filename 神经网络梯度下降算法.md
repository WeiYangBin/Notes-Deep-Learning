## <center>神经网络梯度下降算法</center>
###### <center>2018, SEPT 13</center>

**梯度下降（Gradient Descent)**
是神经网络比较重要的部分，因为我们通常利用梯度来利用**Cost function（成本函数）**
进行**backpropagation（反向传播）**
不断地iteration，更新权重，偏置参数找到损失最低点，然后最终使我们的模型达到更优的效果。

那么梯度下降算法是怎么运作的呢，那就要从Cost function 的计算来谈起，那么Cost function 是什么呢，其实Cost function就是在一次forward propagation时，对所有example的loss function求和。

在这里我们引入一个**交叉熵(cross entropy)成本函数：**


```math
{\displaystyle J(y,a)=-1/n\sum _{x}[y\,\log (a) + (1-y)\,\log (1-a)].\!} {\displaystyle}
```

主要以二分类为例，公式中的y就是train data的label（0，1） ，而a就是前向传播中神经元的值


```math
a = σ(Wx + b) , σ = sigmoid,relu,....
```
σ也就是某一种激活函数,介绍完这些我们需要的东西，那么我们就可以开始进行反向传播梯度下降了。

一个简单梯度前向反向传播的的计算图如下所示：

![image](C:\Users\weiyangbin\Desktop\GD_Compute.png)

然后图片中会有很多如
```math
du =  dJ/dv * dv/dw 
```
这样的公式。

这个是微积分中的**链式法则（chain rule)**，具体不展开解释，既然学习Marchine learning，Deep Learning不会点微积分是不是有点说不过去了，嘿嘿。

接下来，直接上代码：

```
a = 1
b = 2
c = 3

#u,v,J假设是一个神经元
u = a * b
v = a * b + c
J = 2 * (v + b)

print(u, v, J)

#接下去就是疯狂的chain rule了呀
#dv = dJ / dv
dv = 2
#du = dJ / dv  = dJ / du * du / du
du = 2 * 1
#db = dJ / db  = dJ / du * du / db
db = 1 * du
#da = dJ / da  = dJ / du * du / da
da = du * b
#dc = dJ / dc  = dJ / dv * dv / dc
dc = dv * 1

print("dv =", dv)
print("du =", du)
print("dc =", dc)
print("db =", db)
print("da =", da)


```
经过梯度下降算法能得出的每一个parameter的数值，最后只需要执行一下公式便可以完成参数的update，实现Gradient descent algorithm

```math
Parameter = Parameter - (learning_rate) * d(Parameter)  

Parameter = W,b

```

